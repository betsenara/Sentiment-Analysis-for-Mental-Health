{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbcf91b-a231-4e87-974e-6458e0a31396",
   "metadata": {},
   "source": [
    "## First Model - Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095f0a6-dd8d-4e30-be6a-96548effa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('data_clean.csv', encoding='latin-1')\n",
    "\n",
    "# Label encoding the class names\n",
    "encoder = LabelEncoder()\n",
    "data.loc[:, 'status_encoded'] = encoder.fit_transform(data['status'])\n",
    "class_names = encoder.classes_\n",
    "\n",
    "# Adding a new feature: number of words in the stemmed data\n",
    "data['stemmed word count'] = data['stemmed'].apply(len)\n",
    "\n",
    "# Train test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['statement','stemmed','stemmed word count','lemmatized']], data['status_encoded'], test_size=0.2, random_state=42,stratify = data['status_encoded'])\n",
    "\n",
    "# TFIDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2),min_df=10, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['stemmed'])\n",
    "X_test_tfidf = tfidf.transform(X_test['stemmed'])\n",
    "\n",
    "## COMBINE FEATURES\n",
    "\n",
    "# Ensure 'stemmed word count' is reshaped as a 2D array and convert it to sparse format\n",
    "X_train_word_count = X_train[['stemmed word count']].values\n",
    "X_test_word_count = X_test[['stemmed word count']].values\n",
    "\n",
    "# Combine TF-IDF features with the stemmed word count feature\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_word_count])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_word_count])\n",
    "\n",
    "## STACKING CLASSIFIER\n",
    "\n",
    "# Initialize the individual models\n",
    "log_reg = LogisticRegression(max_iter=10000, class_weight='balanced', solver='lbfgs')\n",
    "linear_svc = LinearSVC(C=0.2, class_weight='balanced', dual=False, loss='squared_hinge', max_iter=1000, penalty='l1')\n",
    "nb = MultinomialNB(alpha=0.1, fit_prior=True)\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=20, random_state=42, max_depth=30,\n",
    "                            max_features='sqrt', min_samples_leaf=2, min_samples_split=5, n_estimators=500,max_samples=0.75)\n",
    "xgb_clf = xgb.XGBClassifier(n_jobs=24, random_state=42, colsample_bytree=0.7, learning_rate=0.05, max_depth=4,\n",
    "                            n_estimators=500, reg_alpha=1.0, reg_lambda=0.5, subsample=1.0,gamma=0.1)\n",
    "\n",
    "# Define the base models for stacking\n",
    "estimators = [\n",
    "    ('log_reg', log_reg),\n",
    "    ('linear_svc', linear_svc),\n",
    "    ('nb', nb),\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb_clf)\n",
    "]\n",
    "\n",
    "# The meta-classifier is now an XGBoost classifier\n",
    "meta_classifier = xgb.XGBClassifier(\n",
    "    n_jobs=20,\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.6,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,  # Use a simpler model for the meta-classifier\n",
    "    n_estimators=100,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    subsample=0.75\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create the StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_classifier,\n",
    "    cv=cv,\n",
    "    n_jobs=24\n",
    ")\n",
    "\n",
    "# Train the StackingClassifier\n",
    "stacking_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stacking_clf.predict(X_test_combined)\n",
    "y_pred_train = stacking_clf.predict(X_train_combined)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c47619-98e4-40f1-93c1-e0eeb359ba79",
   "metadata": {},
   "source": [
    "## Second Model - Fine Tuning Bert Sequence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929ea2d8-250e-4c2f-b1f6-f6875a35aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ca05-c9b0-4474-8f63-68291496d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('data_clean.csv', encoding='latin-1')\n",
    "\n",
    "# Text cleaning function for BERT\n",
    "def text_clean_for_bert(text):\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # remove emails\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    # Remove emojis (optional)\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # geometric shapes extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # supplemental arrows\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # chess symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # symbols and pictographs extended\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # We keep punctuation for BERT, no need to tokenize manually\n",
    "    return text.strip()\n",
    "\n",
    "data['bert_clean'] = data['statement'].apply(text_clean_for_bert)\n",
    "\n",
    "# label Encoding\n",
    "encoder = LabelEncoder()\n",
    "data.loc[:, 'status_encoded'] = encoder.fit_transform(data['status'])\n",
    "\n",
    "class_names = encoder.classes_\n",
    "\n",
    "# Train test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_clean'], data['status_encoded'], test_size=0.2, random_state=42,stratify = data['status_encoded'])\n",
    "\n",
    "## TOKENIZE THE DATA\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization function with long labels\n",
    "def tokenize_data(texts, labels, max_len=256):\n",
    "    inputs = tokenizer(\n",
    "        texts.tolist(),  # Convert to list if it's a pandas Series or NumPy array\n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_len, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convert labels to long type\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels.values, dtype=torch.long))\n",
    "    return dataset\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Tokenize X_train and X_test\n",
    "train_dataset = tokenize_data(X_train, y_train)\n",
    "test_dataset = tokenize_data(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "## BERT MODEL FINE TUNING\n",
    "\n",
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_val_loss is None:\n",
    "            self.best_val_loss = val_loss\n",
    "        elif val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0  # Reset the counter if validation loss improves\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  # Stop training if patience is exceeded\n",
    "\n",
    "# Function to evaluate model on a given dataset\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in data_loader:\n",
    "            # Move inputs and labels to device\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            # Make predictions\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "model.config.hidden_dropout_prob = 0.3  # Ensure dropout is set\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define learning rate scheduler (ReduceLROnPlateau)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, min_lr=1e-6, verbose=True)\n",
    "\n",
    "# Instantiate EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=2, min_delta=0.001)\n",
    "\n",
    "# Lists to store loss and accuracy values for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Fine-tuning loop with early stopping and learning rate scheduling\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Append the training loss and accuracy\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # Adjust learning rate manually after 2 epochs\n",
    "    if epoch == 1:\n",
    "        print(\"Reducing learning rate to 1e-6\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-6\n",
    "                \n",
    "    # Adjust learning rate if validation loss plateaus\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# After training, evaluate final performance\n",
    "train_loss, train_accuracy = evaluate(model, train_loader)\n",
    "test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "print(f'Final Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Final Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc54b4d-5c8f-4ebc-98d0-6b4fedb300e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645622a5-7bf0-4118-91d3-160f950297f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
