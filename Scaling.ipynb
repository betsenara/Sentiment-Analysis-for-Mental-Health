{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ae07ca-d786-4489-bd20-adf0bf8f6045",
   "metadata": {},
   "source": [
    "## Scaling of Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b368b5d-fdb3-4ff1-8219-2c66f4715021",
   "metadata": {},
   "source": [
    "The original dataset for this project consisted of mental health-related Reddit data with approximately 50,000 rows. Due to the relatively small size of this dataset, the initial model was trained, tested, and validated using the entire dataset. For the scaling phase of the project, additional data was collected from Reddit. However, privacy regulations made it impossible to obtain new tagged data reflecting the actual mental health conditions of the individuals who wrote the posts. Instead, Reddit data was sourced with tags based on subreddit classifications.\n",
    "\n",
    "Two such datasets were combined to evaluate the performance of the model and its underlying system. The original dataset included seven classes representing specific mental health conditions. In contrast, the newly cleaned and combined dataset contains over 500,000 rows and spans four subreddit categories. For clarity \"Data 1” refers to the original dataset, while “Data 2” refers to the newly acquired dataset.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33914cc-2b08-400b-9f99-e08f64a88945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n",
      "Is CUDA available?: True\n",
      "CUDA device name: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30d13cb-e6ad-42b4-9fe0-e16a4cabf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from time import time\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e0396-a891-46e5-872e-24573914b7cb",
   "metadata": {},
   "source": [
    "## Data Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee4b134-69c5-441b-a812-1b28a5d39e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('data_clean.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f2cb60-e60f-4923-be07-22f2290813af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48249, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c63a89-0ce9-4b7b-a007-1b31db50305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('data2_3combined.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91610cad-228e-48c3-b371-748a9d6b8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528959, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b59bcd-92a5-49ab-adef-95c7fe864ac9",
   "metadata": {},
   "source": [
    "## Clean and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c1fa58-8c8d-443c-80df-37c3d98ea184",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe98b12c-1c2c-4f19-9721-4a6a762886dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for BERT\n",
    "def text_clean_for_bert(text):\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # remove emails\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # geometric shapes extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # supplemental arrows\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # supplemental symbols & pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # chess symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # symbols and pictographs extended\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Regex to remove words with non-ASCII characters\n",
    "    text = re.sub(r'\\b\\w*[^\\x00-\\x7F]+\\w*\\b', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def clean_preprocess_and_split(data):\n",
    "    # Clean data for BERT\n",
    "    data.loc[:,'bert_clean'] = data['statement'].apply(text_clean_for_bert)\n",
    "    \n",
    "    # Eliminates rows where the 'bert_clean' column contains fewer than 2 words after cleaning\n",
    "    data =  data[data['bert_clean'].str.split(' ').apply(lambda x:len(x)>=2)]\n",
    "\n",
    "    # Label encoding\n",
    "    encoder = LabelEncoder()\n",
    "    data.loc[:,'status_encoded'] = encoder.fit_transform(data['status'])\n",
    "    \n",
    "    # Test data split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(data['bert_clean'], data['status_encoded'], test_size=0.15, random_state=42, stratify=data['status_encoded'])\n",
    "    # Train Validation data split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a380bd4-ac7a-428e-bcc6-8c6074da9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\AppData\\Local\\Temp\\ipykernel_3636\\2771571106.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:,'status_encoded'] = encoder.fit_transform(data['status'])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and split dataset 1\n",
    "X_train1, y_train1, X_val1, y_val1, X_test1, y_test1 = clean_preprocess_and_split(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dee3b1-cdb3-4cfc-9c64-417ed50cad58",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be6a116-fab4-44df-87fd-2fd10638eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, batch_size,num_classes):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_name = 'bert-base-uncased'\n",
    "        self.num_classes = num_classes\n",
    "        # Define loss function\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_data(self, tokenizer, texts, labels, max_len=256):\n",
    "        start_time = time()\n",
    "        inputs = tokenizer(\n",
    "            texts.tolist(),  \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len, \n",
    "            return_tensors=\"pt\"\n",
    "            )\n",
    "        tokenization_time = time() - start_time\n",
    "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels.values, dtype=torch.long))\n",
    "        return dataset, tokenization_time\n",
    "\n",
    "    def generate_dataloaders(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        # Load tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Tokenize train and test sets\n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "        y_test = y_test.astype(int)\n",
    "        train_dataset, tokenization_time = self.tokenize_data(tokenizer, X_train, y_train)\n",
    "        val_dataset, _  = self.tokenize_data(tokenizer, X_val, y_val)\n",
    "        test_dataset, _  = self.tokenize_data(tokenizer, X_test, y_test)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader, tokenization_time\n",
    "\n",
    "    def train(self,train_loader,val_loader):\n",
    "        start_time = time()\n",
    "        # Load the pre-trained BERT model\n",
    "        num_labels = self.num_classes\n",
    "        model = BertForSequenceClassification.from_pretrained(self.model_name, num_labels=num_labels)\n",
    "        model.config.hidden_dropout_prob = 0.3  # Ensure dropout is set\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        model.to(self.device)\n",
    "\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Define optimizer with weight decay\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "        # Define learning rate scheduler (ReduceLROnPlateau)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, min_lr=1e-6, verbose=True)\n",
    "\n",
    "        # Instantiate EarlyStopping\n",
    "        early_stopping = EarlyStopping(patience=2, min_delta=0.001)\n",
    "\n",
    "        # Lists to store loss and accuracy values for each epoch\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        # Fine-tuning loop with early stopping and learning rate scheduling\n",
    "        num_epochs = 4\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "            for input_ids, attention_mask, labels in train_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(self.device), attention_mask.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            train_accuracy = total_correct / total_samples\n",
    "\n",
    "            # Append the training loss and accuracy\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_loss, val_accuracy, _ = self.evaluate(model, val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Early stopping check\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            # Adjust learning rate manually after 2 epochs\n",
    "            if epoch == 1:\n",
    "                print(\"Reducing learning rate to 1e-6\")\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-6\n",
    "\n",
    "            # Adjust learning rate if validation loss plateaus\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Clear GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        training_time = time()-start_time\n",
    "        return model,training_time, train_losses, train_accuracies,val_losses, val_accuracies\n",
    "\n",
    "    def evaluate(self, model, data_loader):\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_loss, total_correct, total_samples = 0, 0, 0\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in data_loader:\n",
    "                input_ids, attention_mask, labels = input_ids.to(self.device), attention_mask.to(self.device), labels.to(self.device)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                y_pred.extend(preds.cpu().numpy())  # Move to CPU and convert to NumPy\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = total_correct / total_samples\n",
    "        return avg_loss, accuracy, y_pred\n",
    "\n",
    "    def process(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        train_loader, val_loader, test_loader, tokenization_time = self.generate_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        model, training_time, train_losses, train_accuracies,val_losses, val_accuracies = self.train(train_loader,val_loader)\n",
    "        avg_loss, accuracy, y_pred = self.evaluate(model, test_loader)\n",
    "        return {            \n",
    "            \"tokenization_time\": tokenization_time,\n",
    "            \"training_time\": training_time,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"y_pred\": y_pred,\n",
    "            \"model\": model\n",
    "        }\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17a3814-d0b4-4fe1-b1e9-2333abaece48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_val_loss is None:\n",
    "            self.best_val_loss = val_loss\n",
    "        elif val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0  # Reset the counter if validation loss improves\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True  # Stop training if patience is exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "962a019b-b589-4511-a9fb-b6ef030a662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes1 = data1['status'].value_counts().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade3378b-0e34-4860-a93d-6f5f2141b81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in the first dataset is: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes in the first dataset is: {num_classes1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7df16d-f0cf-4630-9114-a1ebbddc3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate to 1e-6\n",
      "Training Time: 1689.0828523635864\n",
      "Tokenization Time: 33.55921173095703\n",
      "Test Accuracy: 0.8250587747199557\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the ModelTrainer class\n",
    "trainer = ModelTrainer(batch_size=32, num_classes=num_classes1)\n",
    "\n",
    "# Process the splits\n",
    "results = trainer.process(X_train1, y_train1, X_val1, y_val1, X_test1, y_test1)\n",
    "\n",
    "# Access results\n",
    "y_pred = results[\"y_pred\"]\n",
    "print(\"Training Time:\", results[\"training_time\"])\n",
    "print(\"Tokenization Time:\", results[\"tokenization_time\"])\n",
    "print(\"Test Accuracy:\", results[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5556fca-ee2c-4c85-9592-215ce4a2e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model\n",
    "torch.save(results[\"model\"], \"model1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddbd2b-22d5-43f1-af0c-4d546fb47552",
   "metadata": {},
   "source": [
    "## Data 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e87c9a-3a8c-429a-aa83-b19c69e4f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and split dataset 1\n",
    "X_train2, y_train2, X_val2, y_val2, X_test2, y_test2 = clean_preprocess_and_split(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0db278-3a49-43fd-89ee-4600d4fe0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = data2['status'].value_counts().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d849e6b4-e918-450d-91a2-87b53e94fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in the second dataset is: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of classes in the second dataset is: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a1b023d-4fd0-419f-9627-09d1828870eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Betul\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing learning rate to 1e-6\n",
      "Early stopping\n",
      "Training Time: 18687.928438186646\n",
      "Tokenization Time: 523.191891670227\n",
      "Test Accuracy: 0.8306614236741279\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the ModelTrainer class\n",
    "trainer = ModelTrainer(batch_size=32, num_classes=num_classes)\n",
    "\n",
    "# Process the splits\n",
    "results2 = trainer.process(X_train2, y_train2, X_val2, y_val2, X_test2, y_test2)\n",
    "\n",
    "# Access results\n",
    "y_pred2 = results2[\"y_pred\"]\n",
    "print(\"Training Time:\", results2[\"training_time\"])\n",
    "print(\"Tokenization Time:\", results2[\"tokenization_time\"])\n",
    "print(\"Test Accuracy:\", results2[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73803d18-e373-4dad-80bc-5108e79397bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model\n",
    "torch.save(results2[\"model\"], \"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6244e570-3af6-4814-a4a0-63618d41f426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\nmodel2 = torch.load(\"model2.pth\")\\nmodel2.to(device)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2 = torch.load(\"model2.pth\")\n",
    "model2.to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7377e4-4a87-4dbc-9b1b-5b3ec33d66e1",
   "metadata": {},
   "source": [
    "The size of the first dataset is 48,249, and the second dataset is 528,959. There is an approximately 11-fold difference between the sizes of these datasets. The training time for the first model is 1,689 seconds (approximately 28 minutes), and for the second model, it is 18,688 seconds (approximately 5 hours, 11 minutes). The ratio of training times is 11, which matches the ratio of their sizes. \n",
    "The tokenization time for the first model is 33.6 seconds, and for the second model, it is 523 seconds (approximately 9 minutes), with a ratio of approximately 15. The test accuracy of the first model is 0.825, while the second model achieves 0.83.\n",
    "\n",
    "These values suggests that the training process scales linearly with dataset size, indicating that the model handles larger datasets efficiently without any unexpected exponential increase in training time.\n",
    "\n",
    "To further reduce training time, we could have leveraged parallelization with multiple GPUs. However, since we currently have only one GPU, we will maintain the existing system configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0cede-bc6e-4855-a092-0e8671b17440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
